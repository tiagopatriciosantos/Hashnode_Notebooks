{"cells":[{"cell_type":"markdown","id":"bc4e0275-1194-4e1f-9388-7b50bfaa3206","metadata":{"tags":[]},"source":["# Identify fake job postings! - part 2"]},{"cell_type":"markdown","id":"816208ca-f926-4714-9109-d058e8685c38","metadata":{},"source":["Problem statement:\n","\n","> My friend is on the job market. However, they keep wasting time applying for fraudulent job postings. They have asked me to use my data skills to filter out fake postings and save them effort.\n","> They have mentioned that job postings are abundant, so they would prefer my solution to risk filtering out real posts if it decreases the number of fraudulent posts they apply to.\n","> I have access to a dataset consisting of approximately 18'000 job postings, containing both real and fake jobs.\n"]},{"cell_type":"markdown","id":"caca346a-8fff-476c-a26a-714deb7313ea","metadata":{},"source":["## Story published with Jupyter2Hashnode\n","\n","Have you ever struggled to convert a Jupyter Notebook into a compelling Hashnode story? If so, you're not alone. It can be a daunting task, but fortunately, there's a tool that can simplify the process: Jupyter2Hashnode.\n","\n","With Jupyter2Hashnode, you can convert Jupyter Notebooks into Hashnode stories with just a single command. The tool compresses images, uploads them to the Hashnode server, updates image URLs in the markdown file, and finally, publishes the story article. It's an effortless way to transform your data analysis or code tutorials into a polished and engaging format.\n","\n","If you're interested in learning more about Jupyter2Hashnode, there's a detailed guide available on Hashnode (https://tiagopatriciosantos.hashnode.dev/jupyter2hashnode-an-effortless-way-to-convert-jupyter-notebooks-to-hashnode-stories). It's a game-changing tool that can save you time and energy while helping you create high-quality content for your audience. Give it a try and see the difference for yourself!\n"]},{"cell_type":"markdown","id":"5afd4e18-3f50-4800-98e8-709a7ca285c1","metadata":{},"source":["# The part 2\n","\n","This end-2-end ML (Machine Learning) project is divided into a 3 part series.\n","\n","- Part 1 - is all about getting to know the Dataset using Exploratory analysis, cleaning data, choosing the metrics and doing the first model prediction experiments.\n","- Part 2 - is about setup of DagsHub,  DVC and MLFlow to create a version-controlled data science project, as well as tracking experiment parameters and metrics, and comparing experiments.\n","- Part 3 - is all about deployment, where using MLFlow and FastApi we will deploy the model into a WebAPI and serve it with mogenius, a Virtual DevOps platform.\n","\n","\n","Checkout the DagsHub project [here](https://dagshub.com/tiagopatriciosantos/FakeJobPostsProject)."]},{"cell_type":"markdown","id":"54f0a8db-01b1-4399-840c-49c4d97495e2","metadata":{"tags":[]},"source":["## Tools\n","\n","For this part I will use git as and VS Code as editor.\n","\n","Follow the instructions to install:\n","- [Git](https://github.com/git-guides/install-git)\n","- [VS Code](https://code.visualstudio.com/download)\n","\n","I assume to have a working Python 3 installation on local system.\n"]},{"cell_type":"markdown","id":"bf11db65-069c-4b89-a111-8c6902e12323","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["## What is DagsHub?\n","\n","https://dagshub.com/\n","\n","DagsHub is where people build data science projects. A centralized place to host, version, and manage code, data, models, experiments, and more. It allows you and your team to easily share, review, and reuse work, providing a GitHub experience for machine learning. By default DagsHub also provides a MLflow tracking server for repository.\n","\n","\n","I will show the steps that I've used to setup the project, although feel free to follow DagsHub tutorials to get a different understand of the tool:\n","1. https://dagshub.com/docs/experiment_tutorial/\n","2. https://dagshub.com/docs/integration_guide/mlflow_tracking/index.html\n","\n"]},{"cell_type":"markdown","id":"d6780ebb-9caf-434d-8862-c7ac26fe1d18","metadata":{"tags":[]},"source":["## Joining DagsHub...\n","...is really easy. Just sign up. Then, after logging in, create a new repo, simply by clicking on the plus sign and create a repository in the navbar.\n","\n","![create_repo.png](https://dagshub.com/docs/tutorial/assets/create_repo.png)\n","\n"]},{"cell_type":"markdown","id":"389b7bc0-3061-4428-87f7-c57fcd2ff391","metadata":{},"source":["This opens up a dialog, which should be somewhat familiar, in which you can set the repository name, description, and a few other options.\n","\n","![](https://i.imgur.com/JNmJldP.png)"]},{"cell_type":"markdown","id":"40572924-57fb-457e-832f-8b972d2dc1d5","metadata":{},"source":["Let's now clone the repository into our local machine, copying the clone command on Dagshub repository.\n","\n","![](https://i.imgur.com/VtGOjo3.png)\n"]},{"cell_type":"markdown","id":"0bde11c1-eadd-4b7e-8cd5-643c9e11eac5","metadata":{},"source":["Execute this commands in the command line:\n","```console\n","cd path/to/folder\n","git clone https://dagshub.com/tiagopatriciosantos/FakeJobPostsProject.git\n","cd FakeJobPostsProject\n","```\n","\n","With VS Code already installed we can now run:\n","```console\n","code .\n","```\n","\n","That will open the VS Code editor.\n"]},{"cell_type":"markdown","id":"a60cd26e-41d8-49e8-aaae-2a07f30281f9","metadata":{},"source":["## Creating a virtual python environment\n","\n","To create and activate our virtual python environment using venv, type the following commands into your terminal (still in the project folder):\n","\n","\n","Linux/Mac\n","```console\n","python3 -m venv .venv\n","echo .venv/ >> .gitignore\n","source .venv/bin/activate\n","```\n","Windows\n","```powershell\n","python3 -m venv .venv\n","echo .venv/ >> .gitignore\n",".venv\\Scripts\\activate.bat\n","```\n","\n","\n","The first command creates the virtual environment - a directory named .venv, located inside your project directory, where all the Python packages used by the project will be installed without affecting the rest of your computer.\n","\n","The second command activates the virtual python environment, which ensures that any python packages we use don't contaminate our global python installation.\n","\n","The rest of this tutorial should be executed in the same shell session.\n","If exit the shell session or want to create another, we need to make sure to activate the virtual environment in that shell session first.\n","\n","## Installing requirements\n","To install the requirements for the first part of this project, I've created a new file with the name requirements.txt and place the text inside with these direct dependencies:\n","```plaintext\n","dagshub==0.2.9\n","dvc==2.38.1\n","fsspec==2022.11.0\n","joblib==1.2.0\n","pandas==1.5.2\n","scikit-learn==1.2.0\n","typer==0.7.0\n","rich==13.0.0\n","aiohttp==3.8.3\n","mlflow==2.1.1\n","python-dotenv==0.21.1\n","```\n","\n","Now, to install type:\n","```console\n","pip install -r requirements.txt\n","```\n","\n"]},{"cell_type":"markdown","id":"8bbc700f-56d0-4c05-bd7d-41f6198c16c8","metadata":{},"source":["## Downloading the raw data\n","\n","We'll keep our data in a folder named data.\n","\n","It's also important to remember to add this folder to .gitignore! We don't want to accidentally commit large data files to Git.\n","\n","The following commands should take care of everything:\n","\n","```console\n","mkdir -p data\n","echo /data/ >> .gitignore\n","```\n","Linux/Mac\n","```console\n","wget https://dagshub.com/tiagopatriciosantos/Datasets/raw/f3ddde257b100018bcb22a7231f899462b34c58f/data/fake_job_postings.csv -O data/fake_job_postings.csv\n","```\n","Windows powershell\n","```powershell\n","Invoke-WebRequest https://dagshub.com/tiagopatriciosantos/Datasets/raw/f3ddde257b100018bcb22a7231f899462b34c58f/data/fake_job_postings.csv -O data/fake_job_postings.csv\n","```\n","Windows command line\n","```console\n","cd data\n","curl.exe https://dagshub.com/tiagopatriciosantos/Datasets/raw/f3ddde257b100018bcb22a7231f899462b34c58f/data/fake_job_postings.csv -O fake_job_postings.csv\n","```\n","\n","\n","## Committing progress to Git\n","Let's check the Git status of our project:\n","\n","```console\n","$ git status -s\n","?? .gitignore\n","?? requirements.txt\n","```\n","Now let's commit this to Git and push to DagsHub using the command line:\n","\n","```console\n","git add .\n","git commit -m \"Initialized project\"\n","git push -u origin main\n","```\n","\n","You can now see the setup files on your DagsHub repo. So far so good.\n"]},{"cell_type":"markdown","id":"f6b9cd4f-55f4-413b-b29f-00a6a408674a","metadata":{},"source":["## Installing DVC\n","Installing DVC is as simple as To start, we need to initialize our git repo to also use DVC for data versioning:\n","\n","```console\n","dvc init\n","```"]},{"cell_type":"markdown","id":"4e88a4f1-651b-40d5-ab39-2d82e4af83cd","metadata":{},"source":["The following directory structure should be created:\n","\n","```plaintext\n",".dvc\n","â”œâ”€â”€ .gitignore\n","â”œâ”€â”€ config\n","â”œâ”€â”€ plots\n","â”‚   â”œâ”€â”€ confusion.json\n","â”‚   â”œâ”€â”€ default.json\n","â”‚   â”œâ”€â”€ scatter.json\n","â”‚   â””â”€â”€ smooth.json\n","â””â”€â”€ tmp\n","```\n","\n","This is somewhat similar to the .git folder contained in every git repo, except some of its contents will be tracked using git.\n","\n","- .dvc/config is similar to .git/config. By default, it's empty. More on this later on.\n","- .dvc/.gitignore makes sure git ignores DVC internal files that shouldn't be tracked by Git.\n","- .dvc/plots contains predefined templates for plots you can generate using dvc - more info here.\n","- .dvc/tmp is used by DVC to store temporary files, this shouldn't interest the average user.\n","- .dvc/cache doesn't exist yet, but it is where DVC will keep the different versions of our data files. It's very similar in principle to .git/objects.\n","\n","\n","Some of the files generated by dvc init should be tracked by Git, so let's start by committing that:\n","\n","```console\n","git add .dvc\n","git commit -m \"dvc init\"\n","```\n","\n","## Instructing DVC to track data and outputs\n","\n","Let's create a directory to save our outputs, outputs like the ML model we will create and save:\n","```console\n","mkdir -p outputs\n","echo /outputs/ >> .gitignore\n","```\n","Note that our outputs are also in .gitignore - you usually won't want to save these using Git, especially if dealing with large models like neural networks.\n","\n","\n","Now that we have DVC installed, telling it to keep track of our data and outputs is simple with dvc add:\n","\n","```console\n","dvc add data\n","dvc add outputs\n","```\n","\n","You should see two new metadata files, created by DVC:\n","\n","```console\n","$ git status -s\n"," M .gitignore\n","?? data.dvc\n","?? outputs.dvc\n","\n","$ cat data.dvc\n","outs:\n","- md5: 714b1181c5d7cb9dda66272be8be33ac.dir\n","  path: data\n","\n","$ cat outputs.dvc\n","outs:\n","- md5: bc939fd1899e52dd1a5c65be0443986a.dir\n","  path: outputs\n","```\n","\n","Now, we can commit these .dvc files to Git:\n","\n","```console\n","git add data.dvc outputs.dvc\n","git commit -m \"Added data and outputs to DVC\"\n","```\n","From now on, this version of the data and models will be tied to this Git commit, and we'll be able to reproduce them easily later on."]},{"cell_type":"markdown","id":"a513aa5b-9f46-48ee-81df-1675e1620432","metadata":{},"source":["## Writing the code\n","Let's use our existing insights and code from the data exploration level to get started writing code which:\n","- Loads the data\n","- Processes the data\n","- Trains a classification model\n","- Evaluates the trained model and reports relevant metrics.\n","\n","We'll structure our project into the fowling folders and files:\n","\n","```plaintext\n",".\n","â”‚   .dvcignore\n","â”‚   .env    --> File to store local environment variables\n","â”‚   .gitignore\n","â”‚   data.dvc\n","â”‚   LICENSE\n","â”‚   main.py    --> File that is the starting point of our cli application\n","â”‚   outputs.dvc\n","â”‚   README.md\n","â”‚   requirements.txt    --> File that have all the necessary dependencies to make our project run\n","â”œâ”€â”€â”€.dvc\n","â”‚   â”‚   .gitignore\n","â”‚   â”‚   config\n","â”‚   â”œâ”€â”€â”€cache\n","â”‚   â””â”€â”€â”€tmp\n","â”œâ”€â”€â”€custom_code    --> folder to store the custom code like class's\n","â”‚       transformer.py    -> StringConcatTransformer trasnformer that will concat columns\n","â”‚       __init__.py\n","â”œâ”€â”€â”€data    --> folder to store the data\n","â”‚       fake_job_postings.csv\n","â”œâ”€â”€â”€outputs    --> folder to store outputs like the ML model\n","â”œâ”€â”€â”€src    --> folder to store code that executes preprocess, training, model evaluation, ...\n","â”‚   â”‚   constants.py\n","â”‚   â”‚   data_preprocess.py\n","â”‚   â”‚   model.py\n","```\n","\n","\n","You can find the full code at https://dagshub.com/tiagopatriciosantos/FakeJobPostsProject\n","\n","Let's dig into some code explanation:\n"]},{"cell_type":"markdown","id":"717f871c-1aaa-4e0d-8096-abbd68fa12d2","metadata":{},"source":["### `.env`\n","\n","This file stores the necessary environment variables and will be used when calling `load_dotenv()`\n","\n","```plaintext\n","MLFLOW_TRACKING_USERNAME=tiagopatriciosantos\n","MLFLOW_TRACKING_PASSWORD=<secret>\n","```\n","\n","\n","ðŸš©ðŸš¨ Don't forget to include this file in the `.gitignore` file, you don't want to push to your public repository your secrets.\n","```console\n","echo .env >> .gitignore\n","```\n","\n","\n","We can get the necessary MLFlow values from Dagshub repository:\n","\n","![](https://i.imgur.com/yaJNfXf.png)\n"]},{"cell_type":"markdown","id":"a8b90a64-3507-4daa-90fb-ff7b9bfe16b6","metadata":{},"source":["\n","### `main.py`\n","\n","- We call `load_dotenv` to load the necessary environment variables\n","- Using `Repo` we store the current git branch name\n","- We then set `MLFlow` to track our mlflow dagshub uri and the experiment name as our current git branch name, we set also that mlflow will autolog but not the model, we will \"manually\" save the model with some custom information [Dagshub integration guide](https://dagshub.com/docs/integration_guide/mlflow_tracking/index.html#:~:text=logging%20functions.%20.-,2.%20Set%20DagsHub%20as%20the%20remote%20URI,-%C2%B6) | [MLFlow docs](https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=autolog#mlflow.autolog)\n","- Used `Typer` library for building the CLI application [Typer docs](https://typer.tiangolo.com/)\n","- We have created 4 different commands that can be called in the command line, check the docstrings in the code to found out more\n","\n","\n","```python\n","import typer\n","import mlflow\n","from git import Repo\n","from src import data_preprocess\n","from src import model\n","from rich import print\n","\n","## loads environment variables from .env file\n","from dotenv import load_dotenv\n","load_dotenv() \n","\n","## gets the current git local branch name\n","local_repo = Repo(path=\".\")\n","local_branch = local_repo.active_branch.name\n","\n","\n","mlflow.set_tracking_uri(\"https://dagshub.com/tiagopatriciosantos/FakeJobPostsProject.mlflow\")\n","mlflow.set_experiment(local_branch)\n","mlflow.sklearn.autolog(log_models=False)\n","\n","app = typer.Typer()\n","\n","@app.command()\n","def clean():\n","    \"\"\"\n","    This function cleans the raw data from the CSV file.\n","    The cleaned data is saved to a new CSV file.\n","    \"\"\"\n","    data_preprocess.clean()\n"," \n","\n","\n","@app.command()\n","def split():\n","    \"\"\"\n","    Split the data into train and test sets.\n","    This function will create 2 files:\n","        TRAIN_DF_PATH\n","        TEST_DF_PATH\n","    \"\"\"\n","    data_preprocess.split()\n","\n","@app.command()\n","def train():\n","    \"\"\"\n","    This function trains a model on the data in the TRAIN_DF_PATH and TEST_DF_PATH files.\n","    It saves the trained model to the outputs/model.joblib file.\n","    It logs the model's hyperparameters and metrics to DAGsHub.\n","    \"\"\"\n","    model.train()\n","\n","@app.command()\n","def runall():\n","    \"\"\"\n","    Run all the steps of the pipeline.\n","    \"\"\"\n","    data_preprocess.clean()\n","    data_preprocess.split()\n","    model.train()\n","\n","\n","if __name__ == '__main__':\n","    app()\n","\n","```\n","\n"]},{"cell_type":"markdown","id":"1524468f-006f-492d-83f4-9a761b769853","metadata":{},"source":["### `src/data_preprocess.py`\n","\n","This files contains the code to:\n","- Clean the the raw data from the CSV file.It removes non-ascii characters, strips the text and inserts one whitespace between lower capital letter and capitalized letter. The cleaned data is saved to a new CSV file.\n","- Feature engineering\n","- Splits the data into train and test set and writes them to files\n","- For our first experiment we will only select the columns `title`, `description` ,`has_company_logo`\n","    "]},{"cell_type":"markdown","id":"d30f6cf1-bc47-4fc8-b78f-59a55b258153","metadata":{},"source":["### `src/model.py`\n","\n","This file contains the code necessary to build, train and evaluate the model.\n","\n","`_build_model`:\n","- The model is a pipeline were we will have some initial transformers that will work the features to use in our model to train and evaluate\n","- `col_selector` will force to use only the defined columns that we want in this experiment, we need to call `set_output` to \"pandas\" to retain a Dataframe structure with columns names\n","- `StringConcatTransformer` will join the columns  [\"title\",\"description\"] \n","- the column \"has_company_logo\" does not need any transformation as it always have values between 0 and 1\n","- The final estimator in our first experiment will be the Logistic regression\n","\n","The `eval_model` function evaluates a trained model on data set using the average precision, precision, recall, and f1-score metrics.\n","\n","`train`:\n","- the `train` function will load the csv, call our `_build_model` function, train, evaluate and save the model\n","- using the `dagshub_logger` we save the metrics and the `model.joblib` into our outputs folder, we then can upload into our repository this outputs\n","- using mlflow.start_run() as we set the `autolog()` autologging is performed when you call estimator.fit(), estimator.fit_predict() or estimator.fit_transform() and will save the different default metrics\n","- `mlflow.pyfunc.log_model` Log a wrapped custom model as an MLflow artifact for the current run. Because we have a custom transformer in our pipeline and the model wrapper we need to set `code_path` to this custom code so is available when loading this model. Will store all the artifacts under the path named `master` and will register the model as `main` a registered_model_name.\n","[MLFlow docs](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model)\n","\n","```python\n","import dagshub\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from rich import print\n","from .constants import *\n","import joblib\n","import mlflow\n","from mlflow.models import infer_signature\n","from custom_code import StringConcatTransformer, ModelWrapper\n","\n","def _build_model():\n","\n","    cols = [\"title\",\"description\", \"has_company_logo\"]\n","    # transformer to filter columns\n","    col_selector = ColumnTransformer([ (\"cols\", \"passthrough\", cols)  ],remainder=\"drop\",verbose_feature_names_out=False )\n","    col_selector.set_output(transform=\"pandas\")\n","\n","    # transformer to join text columns and apply TfidfVectorizer\n","    text_preprocess = ColumnTransformer(\n","        [(\n","            'processing', \n","            Pipeline([\n","                        (\"concat\", StringConcatTransformer() ), \n","                        (\"vect\",TfidfVectorizer(max_features=25000, ngram_range=(1,2)) )\n","                    ]), \n","            ['title', 'description']\n","        )], \n","        remainder=\"passthrough\")\n","\n","    # create the final pipeline with preprocessing steps and \n","    # the final classifier step\n","    pipeline = Pipeline([\n","        (\"select\", col_selector),\n","        ('text', text_preprocess),\n","        ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=500))\n","    ])\n","\n","    return pipeline\n","\n","def eval_model(estimator, X, y, threshold=0.5):\n","    \"\"\"\n","    Evaluate a model using the metrics:\n","        - average_precision\n","        - precision\n","        - recall\n","        - f1\n","    \n","    Parameters\n","    ----------\n","    estimator : sklearn estimator\n","        The estimator to evaluate.\n","    X : array-like, shape (n_samples, n_features)\n","        The input samples.\n","    y : array-like, shape (n_samples,)\n","        The target values.\n","    \n","    Returns\n","    -------\n","    dict\n","        A dictionary containing the metrics.\n","    \"\"\"\n","    \n","    y_proba = estimator.predict_proba(X)[:,1]\n","\n","    y_pred = y_proba > threshold\n","    \n","\n","    return {\n","        'average_precision': metrics.average_precision_score(y, y_proba),\n","        'precision': metrics.precision_score(y, y_pred),\n","        'recall': metrics.recall_score(y, y_pred),\n","        'f1': metrics.f1_score(y, y_pred)\n","    }\n","\n","def train():\n","    \"\"\"\n","    This function trains a model on the data in the TRAIN_DF_PATH and TEST_DF_PATH files.\n","    It saves the trained model to the outputs/model.joblib file.\n","    It logs the model's hyperparameters and metrics to DAGsHub.\n","    \"\"\"\n","    print('Loading data from files', TRAIN_DF_PATH, TEST_DF_PATH,\"...\")\n","    train_df = pd.read_csv(TRAIN_DF_PATH)\n","    test_df = pd.read_csv(TEST_DF_PATH)\n","\n","    X_train = train_df.drop(columns=[CLASS_LABEL])\n","    y_train = train_df[CLASS_LABEL]\n","    X_test = test_df.drop(columns=[CLASS_LABEL])\n","    y_test = test_df[CLASS_LABEL]\n","\n","    with dagshub.dagshub_logger(metrics_path=\"./outputs/metrics.csv\", hparams_path=\"./outputs/params.yml\") as logger, mlflow.start_run() as run:\n","    \n","        \n","        print('Building the model...')\n","        model = _build_model()\n","\n","        print('Training the model...')\n","        \n","        model.fit(X_train, y_train)\n","        \n","        print('Saving trained model...')\n","        joblib.dump(model, 'outputs/model.joblib')\n","\n","        print(model.get_params())\n","        logger.log_hyperparams({'model': model.get_params()})\n","\n","        model_clf_name = model.get_params()[\"clf\"].__class__.__name__\n","\n","        mlflow.set_tag('estimator_name', model_clf_name )\n","\n","        # based on our analysis we will define this threshold\n","        threshold=0.0485\n","        print('Evaluating model...')\n","        train_metrics = eval_model(model, X_train, y_train, threshold)\n","\n","        print('Threshold:', threshold)\n","        mlflow.set_tag('estimator_threshold', threshold )\n","        threshold_to_log= {\"threshold\":threshold}\n","        logger.log_metrics(threshold_to_log)\n","        mlflow.log_metrics(threshold_to_log)\n","\n","\n","        print('Train metrics:')\n","        print(train_metrics)\n","        metrics_to_log = {f'train__{k}': v for k,v in train_metrics.items()}\n","        logger.log_metrics(metrics_to_log)\n","        mlflow.log_metrics(metrics_to_log)\n","\n","        test_metrics = eval_model(model, X_test, y_test, threshold)\n","        print('Test metrics:')\n","        print(test_metrics)\n","        metrics_to_log = {f'test__{k}': v for k,v in test_metrics.items()}\n","        logger.log_metrics(metrics_to_log)\n","        \n","        mlflow.log_metrics(metrics_to_log)\n","\n","        y_pred =  model.predict_proba(X_test)[:,1]>0.5\n","\n","        signature = infer_signature(X_test, y_pred)\n","\n","        mlflow.pyfunc.log_model(\"master\", \n","                                python_model=ModelWrapper(model, threshold=threshold),\n","                                signature=signature,\n","                                code_path=[\"custom_code\"],\n","                                registered_model_name=\"main\",\n","                                await_registration_for=10,\n","                                input_example = X_test.sample(5, random_state=RANDOM_SEED),\n","                                metadata=threshold_to_log\n","                                )\n","```"]},{"cell_type":"markdown","id":"8916d5d0-12d6-4012-b516-187d770b8e84","metadata":{},"source":["### `custom_code/transformer.py`\n","\n","This file contains the class `StringConcatTransformer`, this class concatenate multiple string fields into a single field, inherits from TransformerMixin, BaseEstimator and ClassNamePrefixFeaturesOutMixin so we can use it in the Pipeline.\n","\n","```python\n","from sklearn.base import TransformerMixin, BaseEstimator, ClassNamePrefixFeaturesOutMixin\n","import numpy as np\n","\n","class StringConcatTransformer(TransformerMixin, BaseEstimator, ClassNamePrefixFeaturesOutMixin):\n","    \"\"\"Concatenate multiple string fields into a single field.\n","    \"\"\"\n","    \n","    def __init__(self, missing_indicator=''):\n","        \"\"\"\n","        NAN value will be replaced by missing_indicator\n","        \"\"\"\n","        self.missing_indicator = missing_indicator\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return np.array(X.fillna(self.missing_indicator).agg(' '.join, axis=1))\n","\n","    def get_feature_names_out(self,input_features=None):\n","        return np.array([\"Text\"])\n","\n","```\n","\n"]},{"cell_type":"markdown","id":"d74a8dc4-f777-4813-98d1-a616725c5885","metadata":{},"source":["`custom_code/model.py`\n","\n","This file contains the class `ModelWrapper`, creates a wrapper for our model.\n","\n","```python\n","from mlflow.pyfunc import PythonModel, PythonModelContext\n","import numpy as np\n","\n","class ModelWrapper(PythonModel):\n","    def __init__(self, model, threshold=0.5):\n","        self._model = model\n","        self._threshold = threshold\n","    \n","    #custom predict function using specific treshold\n","    def predict(self, context: PythonModelContext, data):\n","        return np.array(self._model.predict_proba(data)[:,1]>=self._threshold)\n","\n","```\n"]},{"cell_type":"markdown","id":"b2699e20-9bd9-4b01-a829-e6df71cfeebd","metadata":{},"source":["## Executing the first experiment\n","\n","Running the command we can see the commands we can execute:\n","\n","`python main.py --help`\n","\n","![](https://i.imgur.com/FX0LsXt.png)\n","\n","\n","We will run now the commands, we start with the clean:\n","\n","`python main.py clean`\n","\n","```console\n","Loading data from file data/fake_job_postings.csv ...\n","Cleaning data...\n","Saving data...\n","Clean completed, saved to file data/clean_fake_job_postings.csv.zip\n","```\n","\n","\n","Let's split the data into train and test:\n","\n","`python main.py split`\n","\n","```console\n","Loading data from file data/clean_fake_job_postings.csv.zip ...\n","Saving split data...\n","Split completed, created 2 files data/train.csv.zip data/test.csv.zip\n","```\n","\n","Let's now build the model, train and evaluate:\n","\n","`python main.py train`\n","\n","```console\n","...\n","Model name: main, version 1\n","Created version '1' of model 'main'.\n","```\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"21ccd6cf-335d-42ea-b527-61193240ee7c","metadata":{},"source":["## Committing progress to Git, again\n","\n","So, until now we have made a lot of progress, with last step we have executed the experimentation, that generated files and uploaded info to the remote MLflow server provided by  Dagshub.\n","\n","Let's check the DVC status of our project:\n","```console\n","$ dvc status\n","data.dvc:\n","        changed outs:\n","                modified:           data\n","outputs.dvc:\n","        changed outs:\n","                modified:           outputs\n","```\n","\n","We can see that we have new data and outputs, so we need to commit:\n","To record the md5 of the new model, and save it to .dvc/cache, as well as the data files created when cleanning and splitting the raw Dataset, we now can run:\n","```console\n","dvc commit -f\n","```\n","This updates the outputs.dvc and data.dvc files with the hash of the new files, as well as store in .dvc/cache, let's see them:\n","```console\n","$ cat data.dvc\n","outs:\n","- md5: ea9d5d288c7904bbc412a2064dfa22f9.dir\n","  size: 83661055\n","  nfiles: 4\n","  path: data\n","  \n","$ cat outputs.dvc\n","outs:\n","- md5: 7b0ef7da6f9aa6ce1069530121b7bf7d.dir\n","  size: 32650684\n","  nfiles: 3\n","  path: output\n","```\n","\n","Let's check the Git status of our project:\n","\n","```console\n","$ git status -s\n"," M .gitignore\n"," M requirements.txt\n","?? custom_code/\n","?? data.dvc\n","?? main.py\n","?? outputs.dvc\n","?? src/\n","```\n","\n","Now let's commit this to Git and push to DagsHub using the command line:\n","\n","```console\n","git add .\n","git commit -m \"First LogisticRegression experiment\"\n","```\n"]},{"cell_type":"markdown","id":"8cdb3d06-897d-4a53-b7ee-61c7b298b754","metadata":{},"source":["## Pushing code, data, and models to DagsHub\n","\n","It's great to have saved versions of our data and models in our local workspace, but what if we have team members? Or if we want to continue work on some other machine?\n","\n","DagsHub has you covered - not only can you push your Git code history to DagsHub, but you can also push (and later pull) all DVC managed files!\n","\n","Lets create a script file to ask for the repo, user name and password to store in dvc config files:\n","Linux/Mac script `set_dagshub_repo.sh`\n","```plaintext\n","echo -n \"Username: \"\n","read DAGSHUB_USER\n","echo -n \"Repo name: \"\n","read DAGSHUB_REPO\n","echo -n \"Password: \"\n","read -s DAGSHUB_PASS\n","dvc remote add origin \"https://dagshub.com/$DAGSHUB_USER/$DAGSHUB_REPO.dvc\"  -f\n","dvc remote default origin --local\n","dvc remote modify origin --local user \"$DAGSHUB_USER\"\n","dvc remote modify origin --local auth basic\n","dvc remote modify origin --local password \"$DAGSHUB_PASS\"\n","unset DAGSHUB_PASS\n","```\n","Windows bat file `set_dagshub_repo.bat`\n","```powershell\n","set /p DAGSHUB_USER=\"Username: \"\n","set /p DAGSHUB_REPO=\"Repo name: \"\n","set /p DAGSHUB_PASS=\"Password: \"\n","dvc remote add origin https://DagsHub.com/%DAGSHUB_USER%/%DAGSHUB_REPO%.dvc\"  -f\n","dvc remote default origin\n","dvc remote modify origin --local user %DAGSHUB_USER%\n","dvc remote modify origin --local auth basic\n","dvc remote modify origin --local password %DAGSHUB_PASS%\n","set DAGSHUB_PASS=\n","```\n","\n","We can now execute the script and fill in the values:\n","Linux/Mac\n","```console\n","$ sh set_dagshub_repo.sh\n","\n","```\n","Windows\n","```powershell\n","> .\\set_dagshub_repo.bat\n","```\n","\n","\n","You can see that some DVC stores some configurations in .dvc/config, which should be committed to Git:\n","```console\n","$ git diff\n","diff --git a/.dvc/config b/.dvc/config\n","index e69de29..2d69bfe 100644\n","--- a/.dvc/config\n","+++ b/.dvc/config\n","@@ -0,0 +1,4 @@\n","+[core]\n","+    remote = origin\n","+['remote \"origin\"']\n","+    url = https://DagsHub.com/tiagopatriciosantos/FakeJobPostsProject.dvc\n","```\n","\n","ðŸš©ðŸš¨ Why use --local in the DVC remote commands?\n","> Only configurations that are shared across collaborators should be stored in .dvc/config. The other configuration file is .dvc/config.local - it functions identically to .dvc/config, except it's ignored by Git. That's the correct way to store things like usernames and passwords. We used the --local flag to indicate to DVC that these configuration keys should only be stored locally.\n","> Make sure not to accidentally commit secret information to .dvc/config!\n","\n","\n","So, let's commit these configuration changes to git:\n","\n","```console\n","git add .dvc/config\n","git commit -m \"Configured the DVC remote\"\n","```\n","\n","And push to our repo:\n","```console\n","git push -u origin main\n","dvc push --all-commits\n","```\n","Now, any future collaborator can git clone and then dvc pull the data and models from any version.\n"]},{"cell_type":"markdown","id":"9f41cc26-0f91-4f8d-a043-c0695eba2309","metadata":{},"source":["## Executing a new experiment\n","\n","Now, to run a different experiment, for example using a `RandomForestClassifier`, the best approach is to create a new branch.\n","\n","Now, we can let our imaginations run free with different configurations for experiments.\n","\n","Here are a few examples:\n","\n","- We can change the type of model:\n","    - Random Forest model â€“ model.py with RandomForestClassifier\n","    - Neural Network model â€“ model.py with MLPClassifier\n","- Adding features and onehotencoding\n","- We can play around with parameters:\n","    - We can try out different values for random forest's max_depth parameter â€“ main.py with different max depth\n","- Etc.\n","\n","After each such modification, we'll want to save our code and models. We can do that by running a set of commands like this:\n","\n","```console\n","python3 main.py train\n","dvc commit -f outputs.dvc\n","git checkout -b \"Experiment branch name\"  # It is recommended separating distinct experiments to separate branches. Read more in the note below.\n","git add .\n","git commit -m \"Description of the experiment\"\n","git checkout main\n","```\n","\n","ðŸ—º Branching strategy for experiments\n","\n","> Its often hard to decide what structure to use for your project, and there are no right answers â€“ it depends on your needs and preferences.\n","> Dagshub recommendation is to separate distinct experiments (for example, different types of models) into separate branches, while smaller changes between runs (for example, changing model parameters) are consecutive commits on the same branch.\n","\n","## Pushing our committed experiments to DagsHub\n","To really start getting the benefits of DagsHub, we should now push our Git commit, which captures an experiment and its results, to DagsHub. That will allow us to visualize results.\n","\n","```console\n","# You may be asked for your DagsHub username and password when running this command\n","git push --all\n","dvc push --all-commits\n","```"]},{"cell_type":"markdown","id":"97cbb263-5cb1-4507-aead-717943325662","metadata":{},"source":["## Visualizing experiments on DagsHub\n","\n","To see our experiments visualized, we can navigate to the \"Experiments\" tab in our DagsHub repo:\n","![](https://i.imgur.com/SuYsjm2.png)\n","\n","\n","This table has a row for each detected experiment in your Git history, showing its information and columns for hyperparameters and metrics. \n","Each of these rows corresponds to a experiment train call.\n","\n","You can interact with this table to:\n","- Filter experiments by hyperparameters: Filter experiments by model class\n","- Filter & sort experiments by numeric metric values - i.e. easily find your best experiments: Filter experiments by minimum F1 test score\n","- Choose the columns to display in the table - by default, we limit the number of columns to a reasonable number: Choose displayed columns\n","- Label experiments for easy filtering.\n","- Experiments labeled hidden are automatically hidden by default, but you can show them anyway by removing the default filter. Apply freestyle labels to experiments\n","- Select experiments for comparison.\n","- For example, we can check the top 3 best experiments: Select 3 experiments, then click on the Compare button to see all 3 of them side by side\n","\n"]},{"cell_type":"markdown","id":"eb5b7ff7-eebb-4ba9-8f86-67043653242f","metadata":{},"source":["## MLflow UI\n","\n","The DagsHub MLflow tracking server provides access to the MLflow server user interface (MLflow UI). To view the MLflow UI, visit the tracking server URI (https:\\/\\/dagshub.com/\\<username\\>/\\<repo\\>.mlflow) in a browser. If you haven't interacted with the main DagsHub interface in a while, you may have to enter your DagsHub username and password/access token in to the authentication popup shown by your browser.\n","\n","You should have full access to all views and actions provided by the MLflow UI. This includes viewing run details, comparing runs (within the same experiment only, to compare runs across experiments, visit the DagsHub experiment tracking interface), creating and managing experiments, and viewing and updating the model registry.\n","    \n","We can enter into the MLFlow UI clicking on the button \"Go to mlflow UI\" under the Remote->Experiments \n","![](https://i.imgur.com/us1cqcZ.png)    \n"]},{"cell_type":"markdown","id":"b0dbf907-9809-4f91-bd27-fc4aa28aa13d","metadata":{},"source":["## Part 2 conclusion\n","\n","In this part of the project, the setup of DagsHub, DVC, and MLFlow was demonstrated to create a version-controlled data science project, as well as tracking experiment parameters and metrics and comparing experiments. The steps for creating a virtual Python environment, installing requirements, and downloading raw data were also discussed. Finally, the code to load, process, train, and evaluate a classification model was shown, with environment variables stored in the .env file and tracked using MLFlow. This part of the project demonstrates the importance of using tools like DagsHub, DVC, and MLFlow to simplify data science workflows and ensure reproducibility."]},{"cell_type":"markdown","id":"c4a3bc7f-0eef-4c47-8b9d-3290d0f2b617","metadata":{},"source":["# Next..."]},{"cell_type":"markdown","id":"2de15e44-0050-4186-befc-22e045222af1","metadata":{},"source":["In the next part of this series we will use the MLFlow UI to serve the choosen model, create a WebAPI using FastAPI and deploy using Mogenius."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"toc-autonumbering":true,"toc-showcode":false,"toc-showmarkdowntxt":false,"vscode":{"interpreter":{"hash":"7fcbe07157c1b3b8993a8bcf56b650e6a442ca8e2c193b390cc5479c8aec347c"}}},"nbformat":4,"nbformat_minor":5}
