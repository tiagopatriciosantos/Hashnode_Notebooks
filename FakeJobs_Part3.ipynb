{"cells":[{"source":"# Identify fake job postings! - part 3","metadata":{"tags":[]},"cell_type":"markdown","id":"52b36716-72c1-4bd0-8b09-683b24ef1c15"},{"source":"Problem statement:\n\n> My friend is on the job market. However, they keep wasting time applying for fraudulent job postings. They have asked me to use my data skills to filter out fake postings and save them effort.\n> They have mentioned that job postings are abundant, so they would prefer my solution to risk filtering out real posts if it decreases the number of fraudulent posts they apply to.\n> I have access to a dataset consisting of approximately 18'000 job postings, containing both real and fake jobs.\n","metadata":{},"cell_type":"markdown","id":"da9ebee0-be46-4cb4-91ae-0de471e94082"},{"source":"## Story published with Jupyter2Hashnode\n\nHave you ever struggled to convert a Jupyter Notebook into a compelling Hashnode story? If so, you're not alone. It can be a daunting task, but fortunately, there's a tool that can simplify the process: Jupyter2Hashnode.\n\nWith Jupyter2Hashnode, you can convert Jupyter Notebooks into Hashnode stories with just a single command. The tool compresses images, uploads them to the Hashnode server, updates image URLs in the markdown file, and finally, publishes the story article. It's an effortless way to transform your data analysis or code tutorials into a polished and engaging format.\n\nIf you're interested in learning more about Jupyter2Hashnode, there's a detailed guide available on Hashnode (https://tiagopatriciosantos.hashnode.dev/jupyter2hashnode-an-effortless-way-to-convert-jupyter-notebooks-to-hashnode-stories). It's a game-changing tool that can save you time and energy while helping you create high-quality content for your audience. Give it a try and see the difference for yourself!\n","metadata":{},"cell_type":"markdown","id":"42bc6a4b-7e65-49e1-b5a2-907388c62522"},{"source":"# The part 3\n\nThis end-2-end ML (Machine Learning) project is divided into a 3 part series.\n\n- Part 1 - is all about getting to know the Dataset using Exploratory analysis, cleaning data, choosing the metrics and doing the first model prediction experiments.\n- Part 2 - is about setup of DagsHub,  DVC and MLFlow to create a version-controlled data science project, as well as tracking experiment parameters and metrics, and comparing experiments.\n- Part 3 - is all about deployment, where using MLFlow and FastApi we will deploy the model into a WebAPI and serve it with Mogenius, a Virtual DevOps platform.\n\nYou can check this github project [here.](https://github.com/tiagopatriciosantos/FastApiFakeJobPost)\n\n\n‚ùó‚ö† At time of writing this article Mogenius have suspended new registrations, check their website if they have opened again in the [registration page](https://studio.mogenius.com/user/registration)\n>The public registration is currently suspended due to exceptionally high demand.\n>We will offer it again soon. Contact us if you would like to see a demo of mogenius.","metadata":{},"cell_type":"markdown","id":"727dfefa-59bd-4cd4-903c-2a2679536b42"},{"source":"## Tools\n\nFor this part I will use git as and VS Code as editor.\n\nFollow the instructions to install:\n- [Git](https://github.com/git-guides/install-git)\n- [VS Code](https://code.visualstudio.com/download)\n\nI assume to have a working Python 3 installation on local system.\n\nIt also assumes that we have already logged a model into DagsHub MLflow tracking server.\n","metadata":{"tags":[]},"cell_type":"markdown","id":"475f47f7-a6fc-413c-93d4-99e85a0b4e90"},{"source":"## What is Mogenius?\n\nhttps://mogenius.com/\n\nMogenius is the single layer between your application and the cloud. You can deploy and run any application with mogenius and get it up and running in no time on a hyper-scalable and automated cloud infrastructure. Most application types and services are supported, like web applications, databases, background workers and of course static websites. \n\nRead more about [supported services here](https://docs.mogenius.com/services/service-overview).\n\n\n## For free \n\nWith Community plan we can:\n- Run our personal projects and prototypes on mogenius.\n- Auto-deployment on Kubernetes\n- Hyperscaling cloud resources on AWS or Azure\n- CI/CD pipeline\n- CDN, cybersecurity protection, SSL management\n- Access to the mogenius developer community with monthly free cloud resources and more benefits\n\nWe can compare plans in detail on [pricing page](https://mogenius.com/pricing)\n\n\nI will show the steps that I've used to setup the project, although feel free to follow Mogenius tutorials to have a broader understanding:\n1. https://docs.mogenius.com/getting-started/quickstart\n2. https://docs.mogenius.com/tutorials/how-to-deploy-python-in-the-cloud\n3. https://docs.mogenius.com/tutorials/how-to-deploy-fastapi-in-the-cloud\n\n","metadata":{"tags":[]},"cell_type":"markdown","id":"b608e358-242d-4ca9-a027-cdb78a388e7a"},{"source":"## Joining Mogenius...\n\nFirst, [sign up](https://studio.mogenius.com/user/registration) by entering our email address and choosing a password. Next, verify our email address and phone number to secure our mogenius account. Once completed, we are ready to create our first cloudspace.\n\n\n[![](https://api.mogenius.com/file/id/48f657d6-2032-4b79-95f5-2f15f02e7e4e)](https://studio.mogenius.com/user/registration)\n\n","metadata":{"tags":[]},"cell_type":"markdown","id":"c9ebfc63-6ebc-4cb6-b5cf-ced4dfbb136b"},{"source":"## Create a Cloudspace\n\nStart our first project on mogenius by creating a cloudspace. Give it a name with a maximum of 24 characters, no spaces, or special characters. Click \"Create now\" and our cloudspace will be created using the mogenius Community Plan.\n\n![](https://api.mogenius.com/file/id/d9210359-7406-42f4-8d8f-854205294ce8)\n\nü•≥ Congratulations on creating our first cloudspace on mogenius!","metadata":{},"cell_type":"markdown","id":"eb248b50-3148-4af9-a108-72914c615121"},{"source":"## Add your first service, FastAPI\nOne of the initial tasks is to add services to our cloudspace (e.g. application, database). When we first start, we'll see a pop-up window below. Alternatively, we can add services from our cloudspace dashboard, where we'll also see the available resources in our cloudspace. There are three ways to add services to our cloudspace, we will use a pre-configured service template to create our FastAPI:\n\n![](https://api.mogenius.com/file/id/1d25d25c-2715-4a3e-8201-ec8ceac94cef)\n\n\nWith this option, mogenius will automatically create and add a boilerplate FastAPI template to your Git repository, allowing you to start coding in the newly created repo or to use existing code. Browse the service library or use the search function to find the FastAPI service, then click \"Add service.\"\n\n![](https://i.imgur.com/6saPc3S.png)\n\nNext, if this is the first time you are deploying a service, we need to connect your cloudspace to your repository. Click on ‚ÄúContinue with GitHub,‚Äù which will prompt you to grant permission to access your GitHub repositories. You will only need to do this once, as your mogenius cloudspace will now be connected to your GitHub account and can access your repositories.\n\nNext, we create a new repository by clicking ‚Äú+ Add repository.‚Äù Select a name for the new repository and create it. By default, this will also be the name of our service, but we can also change it to a different name.\n\nWe can leave all settings at default for now, as we can change them at any point later when the service is up and running.\n\nNow, simply click \"Create Service.\" Our FastAPI boilerplate template will be built, added to the specified Git repository, and deployed to our cloudspace simultaneously, allowing to start using it almost immediately. Once the setup routines, build, and deployment process are complete (usually only a few minutes), we can start coding in our repository and access our FastAPI at the specified hostname. Every time we commit any changes to our repository, it will trigger a new build-deploy process automatically (CI/CD).\n\nWe can find all the details on our service's overview page, view metrics, access service logs, add resources, and add additional instances for our service (Kubernetes pods).\n\nThat's it! We have created the FastAPI service, and it will be available to access by other services via the internal hostname that has been assigned to our service, e.g. fastapi-template-8b4tp5:3000. We will choose to expose this service, we will have an external hostname that can be accessed from outside our cloudspace, it looks like this: fastapi-template-prod-myaccount-afooyl.mo2.mogenius.io:80\n\nIf we go to the Github repository we can see the result of this creation:\n![](https://i.imgur.com/rBdT99U.png)\n\n\n\n","metadata":{},"cell_type":"markdown","id":"530637cf-0c64-472c-a369-71ba7a10d400"},{"source":"## MLFlow changing stage to production\n\n\nNow let's put our model in the \"Production\" stage, we will use the production stage model to deploy into our WebAPI.\n\nAccess into the MLFlow UI:\n\n![](https://i.imgur.com/yaJNfXf.png)\n\n\nOpen the model:\n\n![](https://i.imgur.com/7C1dsFD.png)\n\n\nSet into production:\n\n![](https://i.imgur.com/97G507c.png)\n\nSay \"OK\" and voil√°.\n\n![](https://i.imgur.com/yW9kOnn.png)","metadata":{},"cell_type":"markdown","id":"d43472c1-8e68-4c31-a22b-853929ac9024"},{"source":"## Cloning the FastAPI Project\n\nLet's now clone the repository into our local machine, copying the clone command on Github repository.\n\n![](https://i.imgur.com/rBdT99U.png)\n\n\nExecute this commands in the command line:\n```console\ncd path/to/folder\ngit clone https://github.com/tiagopatriciosantos/FastApiFakeJobPost.git\ncd FastApiFakeJobPost\n```\n\nWith VS Code already installed we can now run:\n```console\ncode .\n```\n\nThat will open the VS Code editor.","metadata":{},"cell_type":"markdown","id":"cf8f232d-752e-4ef6-9733-e7ac001ddd01"},{"source":"## Creating a virtual python environment\n\nTo create and activate our virtual python environment using venv, type the following commands into your terminal (still in the project folder):\n\n\nLinux/Mac\n```console\npython3 -m venv .venv\necho .venv/ >> .gitignore\nsource .venv/bin/activate\n```\nWindows\n```powershell\npython3 -m venv .venv\necho .venv/ >> .gitignore\n.venv\\Scripts\\activate.bat\n```\n\n\nThe first command creates the virtual environment - a directory named .venv, located inside your project directory, where all the Python packages used by the project will be installed without affecting the rest of your computer.\n\nThe second command activates the virtual python environment, which ensures that any python packages we use don't contaminate our global python installation.\n\nThe rest of this tutorial should be executed in the same shell session.\nIf exit the shell session or want to create another, we need to make sure to activate the virtual environment in that shell session first.\n\n\n\n## Installing requirements\nTo install the requirements open the requirements.txt and place the text inside with these direct dependencies:\n```plaintext\npydantic>=1.8.0,<2.0.0\nuvicorn==0.20.0\nfastapi==0.89.1\npandas==1.5.3\nscikit-learn==1.2.0\nrich==13.3.0\nmlflow==2.1.1\npython-multipart==0.0.5\npython-dotenv==0.21.1\n```\n\nNow, to install type:\n```console\npip install -r requirements.txt\n```","metadata":{},"cell_type":"markdown","id":"fa5ac6f5-df3e-4374-a191-3cb509954dfe"},{"source":"## Load and serve the model\n\n### `app/main.py`\n\nOpen and put the fowling code into `app/main.py` file.\n\nThis Python code defines a FastAPI application that loads a pre-trained ML model and uses it to make predictions on input data provided by a user through a CSV file.\n\nThe code imports the following modules:\n\n- `FastAPI`: A web framework for building APIs quickly and easily.\n- `File` and `UploadFile` from FastAPI: These are used for handling file uploads in the application.\n- `HTTPException` from FastAPI: This is used to raise HTTP exceptions when there are errors in the application.\n- `mlflow` A machine learning platform for managing the ML lifecycle, including experiment tracking, packaging code into reproducible runs, and sharing and deploying models.\n- `pandas`: A library for data manipulation and analysis.\n- `print` from `rich`: A library for pretty-printing information to the console.\n\nThe code defines a `Model` class to store the pre-trained model and use it for prediction. The `__init__` method of this class loads the deployed model using `mlflow.pyfunc.load_model()` and the `predict` method uses the loaded model to make predictions on new data. The `get_schema` and `get_columns` methods return information about the input schema of the model.\n\nThe code defines a `POST` endpoint with the path `/predict` that accepts a CSV file and returns a JSON object containing the predictions of the model on the input data. If the file is not a CSV file, the application raises an HTTP 400 Exception indicating that only CSV files are accepted.\n\nThe code also defines two `GET` endpoints with the paths `/schema` and `/info` that return information about the input schema and model information, respectively.\n\nFinally, the code creates an instance of the `Model` class using the `main` model name and a tracking URI, sets up the FastAPI application with the initialized `Model` instance, and prints a message indicating that the setup is complete.\n\n```python\nfrom fastapi import FastAPI, File, UploadFile, HTTPException\nimport mlflow\nimport pandas as pd\nfrom rich import print\n\n## loads environment variables from .env file\nfrom dotenv import load_dotenv\nload_dotenv() \n\n# Initialize the FastAPI application\napp = FastAPI(docs_url=\"/\")\n\n# Create a class to store the deployed model & use it for prediction\nclass Model:\n    def __init__(self, model_name: str, tracking_uri ):\n        \"\"\"\n        To initalize the model\n        modelname: Name of the model stored\n        tracking_uri: tracking_uri\n        \"\"\"\n        # Load the deployed model \n        self.model_name = model_name\n        mlflow.set_tracking_uri(tracking_uri)\n        uri =f\"models:/{self.model_name}/Production\"\n        \n        self.model = mlflow.pyfunc.load_model(uri)\n        \n    def predict(self, data):\n        \"\"\"\n        To use the loaded model to make predictions on the data\n        data: Pandas DataFrame to perform predictions\n        \"\"\"\n        predictions = self.model.predict(data)\n        return {  str(k): str(v) for k, v in enumerate(predictions) }\n\n    def get_schema(self, to_dtypes=False):\n        schema = self.model.metadata.signature.inputs.to_dict()\n        if to_dtypes:\n            schema = {r[\"name\"]:  ( r[\"type\"] if r[\"type\"] !=\"string\" else \"object\" )  for r in schema  }\n        return schema\n\n    def get_columns(self):\n        schema = self.model.metadata.signature.inputs.to_dict()\n        return [ r[\"name\"]  for r in schema  ]\n\n    def get_info(self):\n        client = mlflow.MlflowClient()\n        mv = [mv for mv in client.search_model_versions(self.model_name) if mv.current_stage == 'Production' ]\n        return dict(mv[0])\n\nmodel = Model(\"main\",\"https://dagshub.com/tiagopatriciosantos/FakeJobPostsProject.mlflow\")\nprint(\"All setup!\")\n\n# Create the POST endpoint with path '/predict'\n@app.post(\"/predict\", tags=[\"Fake Job\"])\nasync def create_upload_file(file: UploadFile = File(...)):\n    # Handle the file only if it is a CSV\n    if file.filename.endswith(\".csv\"):\n        # CSV file to load the data into a pandas Dataframe\n        data = pd.read_csv(file.file, dtype=model.get_schema(True), usecols=model.get_columns())\n        \n        # Return a JSON object containing the model predictions\n        labels ={\n            \"Labels\": model.predict(data)\n        }\n        return  labels\n    else:\n        # Raise a HTTP 400 Exception, indicating Bad Request \n        raise HTTPException(status_code=400, detail=\"Invalid file format. Only CSV Files accepted.\")\n\n\n@app.get(\"/schema\", tags=[\"Fake Job\"])\nasync def get_schema():\n    return model.get_schema()\n\n@app.get(\"/info\", tags=[\"Fake Job\"])\nasync def get_info():\n    return model.get_info()\n```","metadata":{},"cell_type":"markdown","id":"59ef247f-fbde-4d2d-96dd-8b7779fe7106"},{"source":"\nTo test the code we need to connect into the Dagshub MLFlow server, we can set the environment variables into a `.env` file, as we have `load_dotenv` set up, or we can set environment variables in our command line.\n\n\n### `.env`\n\nThis file stores the necessary environment variables and will be used when calling `load_dotenv()`\n\n```plaintext\nMLFLOW_TRACKING_USERNAME=tiagopatriciosantos\nMLFLOW_TRACKING_PASSWORD=<secret>\n```\n\n\nüö©üö® Don't forget to include this file in the `.gitignore` file, you don't want to push to your public repository your secrets.\n```console\necho .env >> .gitignore\n```\n\n\nWe can get the necessary MLFlow values from Dagshub repository:\n\n![](https://i.imgur.com/yaJNfXf.png)\n\n\n### Test the API\n\nWe can now test the API using the fowling command:\n    \n`uvicorn app.main:app`\n\n```console\nAll setup!\nINFO:     Started server process [28372]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 \n```\n\nWe can now access the address [http://127.0.0.1:8000](http://127.0.0.1:8000) and test our api.\n\n\n    ","metadata":{},"cell_type":"markdown","id":"2edd9dba-5f20-4a1a-821b-65f341d80412"},{"source":"## The docker file\n\n\nWe don't need to make nothing in this file as this already have been setup by Mogenius, the Dockerfile is used to build a Docker image that will run a Python web application using the Uvicorn web server.\n\nHere is a breakdown of the file:\n\n- This line specifies the base image for the Docker image, which is the official Python 3.9 image from Docker Hub.\n`FROM python:3.9`\n\n- This line sets the working directory for the container to /code.\n\n `WORKDIR /code`\n\n- These lines copy the requirements.txt file from the local file system to the container's /code directory, and then installs the Python dependencies specified in the requirements.txt file using pip.\n\n```console\nCOPY ./requirements.txt /code/requirements.txt\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n```\n\n- This line copies the app directory from the local file system to the container's /code/app directory.\n\n`COPY ./app /code/app`\n\n- This line specifies that the container will listen on port 8080. However, this does not actually publish the port - it just documents that the container will use it.\n`EXPOSE 8080`\n\n- This line sets the user to run the container as to 1000. This is useful for security purposes, as it helps to ensure that the container runs with minimal privileges.\n`USER 1000`\n\n- This line specifies the command that should be run when the container starts. It runs the Uvicorn web server with the app.main:app module as the application, and binds to the container's port 8080.\n\n`CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]`\n\n\nThe `Dockerfile` file commands:\n```dockerfile\nFROM python:3.9\n \nWORKDIR /code\n \nCOPY ./requirements.txt /code/requirements.txt\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\nCOPY ./app /code/app\n\nEXPOSE 8080\n\nUSER 1000\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n```\n\n","metadata":{},"cell_type":"markdown","id":"35fd9bed-8032-411f-95d0-a42a8f894847"},{"source":"## Committing progress to Git\n\nCreate the file `.gitignore` and past this text inside:\n```\n.venv\n__pycache__\n.env\n```\n\nLet's check the Git status of our project:\n\n```console\n$ git status -s\n M app/main.py\n M requirements.txt\n?? .gitignore\n```\n\nNow let's commit this to Git using the command line:\n\n```console\ngit add .\ngit commit -m \"Added MLFlow, serve the model logic and endpoint\"\ngit push -u origin main\n```\n\n","metadata":{},"cell_type":"markdown","id":"89a871ee-8ad0-4525-9c0c-0ed9e87535d0"},{"source":"\n","metadata":{},"cell_type":"markdown","id":"36878989-d09e-4fa9-a5ce-93a2abd0b4d7"},{"source":"## Mogenius Environment variables & secrets\n\nYou can define environment variables and secrets using Mogenius UI. \nEach secret is encrypted and then stored in the key vault. \nTo use a particular secret call its name to get the encrypted key, this way, a secret is never written in code, but retrieved from the Key Vault in a secure way.\n\nWe need to create this environment variables so our code can run:\n```plaintext\nMLFLOW_TRACKING_USERNAME=tiagopatriciosantos\nMLFLOW_TRACKING_PASSWORD=<secret>\n```\n\nGo to Mogenius studio and add this into the service environment variables:\n\n![](https://i.imgur.com/e7Y6ANO.png)\n\n\n","metadata":{},"cell_type":"markdown","id":"5b2228d1-50f9-44eb-a625-fc0b5d63c93b"},{"source":"## Checking the final result\n\n\nWe can now go to the external address of our service and use our API...\n\n![](https://i.imgur.com/XevmHKS.png)\n\nAnd test the example file available [here](https://github.com/tiagopatriciosantos/FastApiFakeJobPost/blob/6b7ee11a9fc92ed3b642b67b4bfec69429fbcf83/fake_job_postings_test.csv)\n\n![](https://i.imgur.com/Ze8knw3.png)\n\n\n\n","metadata":{},"cell_type":"markdown","id":"99736751-a875-4817-8444-305acc0a5a4a"},{"source":"# Conclusion\n\nThe conclusion of this end-to-end ML project highlights the three-part series in which the project is divided. Part 1 covers the exploratory analysis, data cleaning, metric selection, and initial model prediction experiments. Part 2 focuses on setting up DagsHub, DVC, and MLFlow for version control, tracking experiment parameters and metrics, and comparing experiments. Finally, Part 3 focuses on deployment, where MLFlow and FastAPI are used to deploy the model into a WebAPI and serve it with Mogenius, a Virtual DevOps platform. The three parts of the project work together to provide a comprehensive overview of end-to-end ML development, from data exploration to deployment.","metadata":{},"cell_type":"markdown","id":"56d48cdf-37d4-43c9-8d6e-4eae184a97f4"},{"source":"","metadata":{},"cell_type":"code","id":"538564f8-49ea-406f-a9c1-44792325a518","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":""},"kernelspec":{"display_name":"","name":""}},"nbformat":4,"nbformat_minor":5}